{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1:\n",
        "\n",
        "What is the fundamental idea behind ensemble techniques? How does bagging differ from boosting in terms of approach and objective?"
      ],
      "metadata": {
        "id": "6FIhle-Qv5I0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "Ensemble techniques combine predictions from multiple models to improve accuracy, stability, and generalization. The key idea is that a group of weak learners together can form a strong learner.\n",
        "Bagging (Bootstrap Aggregating) creates multiple independent models using random subsets of training data, then averages or votes their outputs to reduce variance and prevent overfitting.\n",
        "Boosting, on the other hand, builds models sequentially. Each new model focuses more on correcting the errors of the previous one, thus reducing bias. Bagging aims to lower variance, while boosting primarily reduces bias."
      ],
      "metadata": {
        "id": "SLZAroP_wBlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2:\n",
        "\n",
        "Explain how the Random Forest Classifier reduces overfitting compared to a single decision tree. Mention the role of two key hyperparameters in this process."
      ],
      "metadata": {
        "id": "tyCqwX8qwEsM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "A single decision tree often overfits because it learns every detail of the training data. Random Forest reduces overfitting by creating multiple decision trees using random samples and random subsets of features. The final prediction is made through majority voting, which improves generalization.\n",
        "Two key hyperparameters are:\n",
        "\n",
        "n_estimators – the number of trees; more trees improve performance but increase computation.\n",
        "\n",
        "max_features – limits the number of features each tree can use, ensuring diversity among trees and reducing correlation between them."
      ],
      "metadata": {
        "id": "gf0LG3WkwIm2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3:\n",
        "\n",
        "What is Stacking in ensemble learning? How does it differ from traditional bagging/boosting methods? Provide a simple example use case."
      ],
      "metadata": {
        "id": "vgZqfWXPwMc5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "Stacking (Stacked Generalization) combines predictions from different models (called base learners) using a meta-model that learns the best way to combine their outputs.\n",
        "Unlike bagging and boosting, which use the same model type multiple times, stacking uses different algorithms (like Logistic Regression, Decision Tree, and SVM together).\n",
        "Example: In a loan prediction problem, we can use Random Forest, Gradient Boosting, and Logistic Regression as base models, and a Meta Learner (e.g., XGBoost) to combine their predictions for better accuracy."
      ],
      "metadata": {
        "id": "fPg0NSd8wPHU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4:\n",
        "\n",
        "What is the OOB Score in Random Forest, and why is it useful? How does it help in model evaluation without a separate validation set?"
      ],
      "metadata": {
        "id": "hTbt-FvRwStd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "The Out-of-Bag (OOB) Score is an internal validation metric in Random Forests. When building each tree, only a random subset of samples is used (bootstrap sampling), leaving some samples unused (out-of-bag). These OOB samples are then used to test the tree’s performance.\n",
        "This allows the Random Forest to estimate prediction accuracy without needing a separate validation dataset, saving time and ensuring efficient use of data. It acts as a built-in cross-validation method."
      ],
      "metadata": {
        "id": "Cyi5phztwV1e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5:\n",
        "\n",
        "Compare AdaBoost and Gradient Boosting in terms of:\n",
        "\n",
        "How they handle errors\n",
        "\n",
        "Weight adjustment mechanism\n",
        "\n",
        "Typical use cases"
      ],
      "metadata": {
        "id": "W5wmA6JfwZEv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "\n",
        "Handling errors: AdaBoost adjusts the weights of misclassified samples so the next weak learner focuses more on difficult cases. Gradient Boosting fits new learners to the residual errors of previous models.\n",
        "\n",
        "Weight adjustment: AdaBoost changes sample weights explicitly, while Gradient Boosting minimizes a differentiable loss function using gradient descent.\n",
        "\n",
        "Use cases: AdaBoost is simple and effective for binary classification tasks; Gradient Boosting (like XGBoost or LightGBM) is used for more complex, large-scale regression and classification problems."
      ],
      "metadata": {
        "id": "9H0xPW5KwcQm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6:\n",
        "\n",
        "Why does CatBoost perform well on categorical features without requiring extensive preprocessing? Briefly explain its handling of categorical variables."
      ],
      "metadata": {
        "id": "e0IZBY8xwhvR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "CatBoost handles categorical features internally using target-based encoding and ordered boosting, eliminating the need for manual label or one-hot encoding.\n",
        "It converts categorical values into numerical statistics (like average target value) in a way that prevents data leakage. Its built-in algorithm efficiently manages categorical variables, saving preprocessing time and maintaining model accuracy, especially on datasets with many categorical columns."
      ],
      "metadata": {
        "id": "fvBTqSwDwlJ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7:\n",
        "\n",
        "KNN Classifier Assignment: Wine Dataset Analysis with Optimization"
      ],
      "metadata": {
        "id": "u3J_Pq-bwoZv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "\n",
        "The Wine dataset is loaded and split (70% train, 30% test).\n",
        "\n",
        "A default KNN (K=5) is trained without scaling and evaluated using accuracy, precision, recall, and F1-score.\n",
        "\n",
        "After applying StandardScaler, performance usually improves as KNN is sensitive to feature scale.\n",
        "\n",
        "GridSearchCV is used to find the best K (1–20) and distance metric (Euclidean or Manhattan).\n",
        "\n",
        "The optimized model generally achieves the highest accuracy, showing that scaling and tuning hyperparameters significantly improve KNN performance."
      ],
      "metadata": {
        "id": "HvmH-zbYw3fV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8:\n",
        "\n",
        "PCA + KNN with Variance Analysis and Visualization"
      ],
      "metadata": {
        "id": "oOg9DO0Rw43I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "\n",
        "The Breast Cancer dataset is loaded and PCA is applied to reduce dimensionality.\n",
        "\n",
        "The scree plot shows explained variance ratio for each component.\n",
        "\n",
        "PCA components retaining 95% variance are selected to transform the data.\n",
        "\n",
        "KNN is trained on both the original and PCA-transformed data; results often show slightly reduced accuracy with PCA but faster computation.\n",
        "\n",
        "Finally, the first two principal components are plotted as a scatter plot (colored by class), showing class separation and visualization of data structure."
      ],
      "metadata": {
        "id": "-NHdZlP1w85H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9:\n",
        "\n",
        "KNN Regressor with Distance Metrics and K-Value Analysis"
      ],
      "metadata": {
        "id": "LQZS7Inbw_3R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "\n",
        "A synthetic dataset is generated using make_regression().\n",
        "\n",
        "KNN regressors are trained with Euclidean and Manhattan distances (K=5) and compared using Mean Squared Error (MSE).\n",
        "\n",
        "MSE is typically lower for Euclidean distance when features are scaled properly.\n",
        "\n",
        "Testing multiple K values (1, 5, 10, 20, 50) shows that small K values lead to low bias but high variance, while larger K values increase bias but reduce variance. The K vs MSE plot visualizes this tradeoff clearly."
      ],
      "metadata": {
        "id": "71KCkyuyxFnv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10:\n",
        "\n",
        "KNN with KD-Tree/Ball Tree, Imputation, and Real-World Data"
      ],
      "metadata": {
        "id": "8mh8sJ5IxLHP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "\n",
        "The Pima Indians Diabetes dataset is loaded, and missing values are filled using KNN Imputer.\n",
        "\n",
        "KNN models are trained using three algorithms: Brute-force, KD-Tree, and Ball Tree.\n",
        "\n",
        "KD-Tree and Ball Tree are faster for high-dimensional data than brute force.\n",
        "\n",
        "Accuracy is similar across methods, but KD-Tree or Ball Tree usually train faster.\n",
        "\n",
        "Finally, using the two most important features, the decision boundary is plotted for the best method (often KD-Tree), showing clear class separation and efficient computation."
      ],
      "metadata": {
        "id": "Y4Oq3Tx6xSuR"
      }
    }
  ]
}